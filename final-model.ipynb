{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model\n",
    "\n",
    "## Structure\n",
    "\n",
    "Our final model will include\n",
    "\n",
    "- XGBoost best classifier submission\n",
    "- Random Forest classifier\n",
    "- Logistic regression override *specifically* for predicting the minority class\n",
    "\n",
    "Let's import some stuff and load the data first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = 999\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, precision_score\n",
    "from sklearn.metrics import recall_score, f1_score, confusion_matrix\n",
    "from sklearn.metrics import classification_report, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV, validation_curve, learning_curve\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import category_encoders as ce\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_installer(installer):\n",
    "    \n",
    "    unknown = ['0', 'unknown']\n",
    "    \n",
    "    if installer in unknown:\n",
    "        return 'unknown'\n",
    "    \n",
    "    government = ['government ', 'government', 'dwe', 'hesawa', 'rwe', 'central government', 'lga',\n",
    "                 'district council', 'gover', 'gove', 'gov', 'district water department',\n",
    "                 'sengerema water department', 'distri', 'centr', 'distric water department',\n",
    "                 'tasaf']\n",
    "    \n",
    "    if installer in government:\n",
    "        return 'government'\n",
    "    \n",
    "    community = ['community', 'commu', 'villagers', 'twesa']\n",
    "    \n",
    "    if installer in community:\n",
    "        return 'community'\n",
    "    \n",
    "    religious = ['church of disciples', 'kkkt', 'world vision', 'rc church', 'rc', 'tcrs',\n",
    "                'dmdd']\n",
    "    \n",
    "    if installer in religious:\n",
    "        return 'religious'\n",
    "    \n",
    "    international = ['norad', 'fini water', 'danida', 'danid', 'ces', 'kuwait',\n",
    "                    'finw']\n",
    "    \n",
    "    if installer in international:\n",
    "        return 'international'\n",
    "    \n",
    "    private = ['private', 'privat', 'kiliwater', 'wedeco']\n",
    "    \n",
    "    if installer in private:\n",
    "        return 'private'\n",
    "    \n",
    "    aid = ['roman', 'amref', 'world bank', 'unicef', 'oxfam']\n",
    "    \n",
    "    if installer in aid:\n",
    "        return 'aid'\n",
    "    \n",
    "    \n",
    "    return 'other'\n",
    "\n",
    "def season(month):\n",
    "    \"\"\"\n",
    "    Returns a string corresponding to the typical seasononal period in Tanzania, given\n",
    "    month input as an integer\n",
    "    \"\"\"\n",
    "    if month in [4,5]:\n",
    "        return 'heavy_rain'\n",
    "    elif month in [12, 1, 2]:\n",
    "        return 'hot_dry'\n",
    "    elif month == 3:\n",
    "        return 'intermittent_rain'\n",
    "    elif month in [6,7,8,9,10]:\n",
    "        return 'cool_dry'\n",
    "    else:\n",
    "        return 'short_rains'\n",
    "\n",
    "def wrangle(X, X_train):\n",
    "    \"\"\"\n",
    "    Takes in the raw water pump features and returns an enhanced dataframe\n",
    "    \n",
    "    We also need to pass in the training data for some functions, such as getting top value_counts.\n",
    "    \n",
    "    When wrangling the training set, we will pass two copies of the training data\n",
    "    \n",
    "    \"\"\"\n",
    "    X = X.copy()\n",
    "    X_train = X_train.copy()\n",
    "    \n",
    "    ### date recorded ###\n",
    "    X['year_recorded'] = X['date_recorded'].apply(lambda x: int(x.split('-')[0]))\n",
    "    X['years_since_construction'] = [record - construction if construction > 1900 \n",
    "                                        else 100 for record, construction in \n",
    "                                        zip(X['year_recorded'], X['construction_year'])]\n",
    "    \n",
    "    days = X_train['date_recorded'].value_counts()\n",
    "    X['day_record_count'] = [days[day] if day in days.index else 0 for day in X['date_recorded']]\n",
    "    X['month_recorded'] = X['date_recorded'].apply(lambda x: int(x.split('-')[1]))\n",
    "    X['season'] = X['month_recorded'].apply(season)\n",
    "    \n",
    "    ### funder / installer ###\n",
    "    # convert all strings to lowercase\n",
    "    X['funder'], X['installer'] = X['funder'].str.lower(), X['installer'].str.lower()\n",
    "    X['funded_and_installed'] = np.where(X['funder'] == X['installer'], True, False)\n",
    "    # encode into categories\n",
    "    X['funder_cat'] = X['funder'].apply(map_installer)\n",
    "    X['installer_cat'] = X['installer'].apply(map_installer)\n",
    "    \n",
    "    # drop full feature sets\n",
    "    X = X.drop(['funder', 'installer'], axis=1)\n",
    "    \n",
    "#     top_funders, top_installers = X_train['funder'].str.lower().value_counts().index[:20], X_train['installer'].str.lower().value_counts().index[:20]\n",
    "#     X['funder'] = ['other' if funder not in top_funders else funder for funder in X['funder']]\n",
    "#     X['installer'] = ['other' if installer not in top_installers else installer for installer in X['installer']]\n",
    "    \n",
    "    ### total static head ###\n",
    "    X['tsh_zero'] = X['amount_tsh'] == 0.0\n",
    "#     X['log_amount_tsh'] = np.log(X['amount_tsh']) <- has problems with 0\n",
    "    \n",
    "    ### waterpoint name ###\n",
    "    X['wpt_name'] = X['wpt_name'].str.lower()\n",
    "    top_wpt = X_train['wpt_name'].str.lower().value_counts().index[:50]\n",
    "    X['wpt_name'] = ['other' if wpt_name not in top_wpt else wpt_name for wpt_name in X['wpt_name'].str.lower()]\n",
    "    \n",
    "    ### population ###\n",
    "    X['population_unknown'] = X['population'] == 0\n",
    "    X['population'].replace(0, np.nan, inplace=True)\n",
    "    X[\"population\"].fillna(X_train.groupby(['region', 'district_code'])[\"population\"].transform(\"median\"), inplace=True)\n",
    "\n",
    "    X[\"population\"].fillna(X_train.groupby(['region'])[\"population\"].transform(\"median\"), inplace=True)\n",
    "\n",
    "    X[\"population\"].fillna(X_train[\"population\"].median(), inplace=True)\n",
    "\n",
    "    \n",
    "    ### subvillage ###\n",
    "    # encode unknown as binary\n",
    "    X['subvillage_unknown'] = X['subvillage'].isnull()\n",
    "    # encode top 50, everything else as other\n",
    "    X['subvillage'] = X['subvillage'].str.lower()\n",
    "    sub_villages_count = X_train['subvillage'].str.lower().value_counts()\n",
    "    X['subvillage_waterpoints'] = [sub_villages_count[vill] if vill in sub_villages_count.index else 0 for vill in X['subvillage'].str.lower()]\n",
    "    \n",
    "    top_subvillage = X_train['subvillage'].str.lower().value_counts().index[:50]\n",
    "    X['subvillage'] = ['other' if vill not in top_subvillage else vill for vill in X['subvillage'].str.lower()]\n",
    "    \n",
    "    ### region code ###\n",
    "    # encode region codes as strings, which will be automatically one hot encoded later\n",
    "    X['region_code'] = X['region_code'].astype(str)\n",
    "    X = X.drop('region', axis=1)\n",
    "    \n",
    "    ### district code ###\n",
    "    # encode district codes as strings, which will be automatically one hot encoded later\n",
    "    X['district_code'] = X['district_code'].astype(str)\n",
    "    \n",
    "    ### lga ###\n",
    "    X['lga'] = X['lga'].str.lower()\n",
    "    # encode urban and rural\n",
    "    X['lga_rural'] = [lga.find('rural') != -1 for lga in X['lga']]\n",
    "    X['lga_urban'] = [lga.find('urban') != -1 for lga in X['lga']]\n",
    "    # encode anything except the top 30 as other\n",
    "    top_lga = X_train['lga'].str.lower().value_counts().index[:30]\n",
    "    X['lga'] = ['other' if lga not in top_lga else lga for lga in X['lga'].str.lower()]\n",
    "    \n",
    "    ### ward ###\n",
    "    # create new variable with the number of waterpoints in a given ward\n",
    "    ward_counts = X_train['ward'].str.lower().value_counts()\n",
    "    X['ward_wpt_count'] = [ward_counts[ward] if ward in ward_counts.index else 0 for ward in X['ward'].str.lower()]\n",
    "    \n",
    "    \n",
    "    ### public meeting ###\n",
    "    X['public_meeting_unknown'] = X['public_meeting'].isnull() # encode missing vals to a binary var\n",
    "    X['public_meeting'] = X['public_meeting'].fillna(False) # fill na with false\n",
    "    \n",
    "    ### recorded by ###\n",
    "    X = X.drop('recorded_by', axis=1) # drop it, all the same value\n",
    "    \n",
    "    ### permit ###\n",
    "    X['permit_unknown'] = X['permit'].isnull() # encode missing vals to a binary var\n",
    "    X['permit'] = X['permit'].fillna(False) # fill na with false\n",
    "    \n",
    "    ### scheme name/management ###\n",
    "    X['scheme_management'] = X['scheme_management'].fillna('None') # encode nulls as 'None'\n",
    "    X = X.drop('scheme_name', axis=1)\n",
    "    \n",
    "    ### construction year ###\n",
    "    X['construction_unknown'] = X['construction_year'] < 1900\n",
    "#     X['early60s'] = X['construction_year'].between(1900, 1964)\n",
    "#     X['late60s'] = X['construction_year'].between(1965, 1969)\n",
    "#     X['early70s'] = X['construction_year'].between(1970, 1974)\n",
    "#     X['late70s'] = X['construction_year'].between(1975, 1979)\n",
    "#     X['early80s'] = X['construction_year'].between(1980, 1984)\n",
    "#     X['late80s'] = X['construction_year'].between(1985, 1989)\n",
    "#     X['early90s'] = X['construction_year'].between(1990, 1994)\n",
    "#     X['late90s'] = X['construction_year'].between(1995, 1999)\n",
    "#     X['early00s'] = X['construction_year'].between(2000, 2004)\n",
    "#     X['late00s'] = X['construction_year'].between(2005, 2009)\n",
    "#     X['early10s'] = X['construction_year'].between(2010, 2014)\n",
    "#     X = X.drop('construction_year', axis=1)\n",
    "    \n",
    "    ### extraction type ###\n",
    "    X = X.drop(['extraction_type_group', 'extraction_type_class'], axis=1)\n",
    "    \n",
    "    ### management ###\n",
    "    X = X.drop('management_group', axis=1)\n",
    "    \n",
    "    ### payment ###\n",
    "    X = X.drop('payment_type', axis=1)\n",
    "    \n",
    "    ### quality ###\n",
    "    X = X.drop(['quality_group', 'quantity_group'], axis=1)\n",
    "    \n",
    "    ### source ###\n",
    "    X = X.drop(['source_type', 'source_class'], axis=1)\n",
    "    \n",
    "    ### waterpoint type ###\n",
    "    X = X.drop('waterpoint_type_group', axis=1)\n",
    "    \n",
    "    ### longitude/latitude/gps_height ###\n",
    "    X['gps_height_bad'] = X['gps_height'] <= 0.0 # min height should be sea level\n",
    "    X['latitude_bad'] = X['latitude'] < 25.0\n",
    "    X['longitude_bad'] = X['longitude'] > -0.5\n",
    "    \n",
    "    # fill in missing values based on location - STOLEN FROM BREADWARD\n",
    "    \n",
    "#     training_data[\"gps_height\"].fillna(training_data.groupby(['region', 'district_code'])[\"gps_height\"].transform(\"mean\"), inplace=True)\n",
    "#     training_data[\"gps_height\"].fillna(training_data.groupby(['region'])[\"gps_height\"].transform(\"mean\"), inplace=True)\n",
    "#     training_data[\"gps_height\"].fillna(training_data[\"gps_height\"].mean(), inplace=True)\n",
    "#     training_data[\"amount_tsh\"].fillna(training_data.groupby(['region', 'district_code'])[\"amount_tsh\"].transform(\"median\"), inplace=True)\n",
    "#     training_data[\"amount_tsh\"].fillna(training_data.groupby(['region'])[\"amount_tsh\"].transform(\"median\"), inplace=True)\n",
    "#     training_data[\"amount_tsh\"].fillna(training_data[\"amount_tsh\"].median(), inplace=True)\n",
    "#     training_data[\"latitude\"].fillna(training_data.groupby(['region', 'district_code'])[\"latitude\"].transform(\"mean\"), inplace=True)\n",
    "#     training_data[\"longitude\"].fillna(training_data.groupby(['region', 'district_code'])[\"longitude\"].transform(\"mean\"), inplace=True)\n",
    "#     training_data[\"longitude\"].fillna(training_data.groupby(['region'])[\"longitude\"].transform(\"mean\"), inplace=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ### DROPPING AFTER FEATURE ENGINEERING ####\n",
    "    X = X.drop('date_recorded', axis=1)\n",
    "  \n",
    "    return X # returned wrangled dataframe\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "\n",
    "Next, let's load in our best XGBoost model and run predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data prep for XGBoost\n",
    "df_raw = pd.read_csv('train_features.csv', index_col=0)\n",
    "df_test_raw = pd.read_csv('test_features.csv', index_col=0)\n",
    "X_train, X_test = wrangle(df_raw, df_raw), wrangle(df_test_raw, df_raw)\n",
    "y_train = pd.read_csv('train_labels.csv', index_col=0)\n",
    "# encoding categoricals\n",
    "cat_encoder = ce.OneHotEncoder(use_cat_names=True)\n",
    "cat_encoder.fit(X_train, y_train)\n",
    "X_train_clean = cat_encoder.transform(X_train)\n",
    "X_test_clean = cat_encoder.transform(X_test)\n",
    "# prepping all data, we should only be using dtest for predictions\n",
    "# encode labels\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "xgb_le = LabelEncoder()\n",
    "y_train_clean = xgb_le.fit_transform(y_train.values.ravel())\n",
    "\n",
    "X_train_clean_train, X_train_clean_val, y_train_clean_train, y_train_clean_val = train_test_split(X_train_clean,\n",
    "                                                                                                 y_train_clean,\n",
    "                                                                                                 test_size=0.10,\n",
    "                                                                                                 stratify=y_train_clean,\n",
    "                                                                                                 shuffle=True,\n",
    "                                                                                                 random_state=420)\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train_clean_train, label=y_train_clean_train)\n",
    "dval = xgb.DMatrix(X_train_clean_val, label=y_train_clean_val)\n",
    "dtest = xgb.DMatrix(X_test_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model \n",
    "# v19 best iteration thusfar\n",
    "xgb_best = xgb.Booster(model_file='xgboost_iteration21_v0.model')\n",
    "# predict validation and test data\n",
    "xgb_test, xgb_val = xgb_best.predict(dtest), xgb_best.predict(dval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier\n",
    "\n",
    "Let's implement a random forest classifier to compliment our other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and wrangle data\n",
    "df_raw = pd.read_csv('train_features.csv', index_col=0)\n",
    "df_test_raw = pd.read_csv('test_features.csv', index_col=0)\n",
    "X_train, X_test = wrangle(df_raw, df_raw), wrangle(df_test_raw, df_raw)\n",
    "y_train = pd.read_csv('train_labels.csv', index_col=0)\n",
    "\n",
    "#  splitting data - ONE SUBMISSION ON FULL TRAINING SET\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train,\n",
    "                                                 y_train,\n",
    "                                                 test_size=0.10,\n",
    "                                                 stratify=y_train,\n",
    "                                                 shuffle=True,\n",
    "                                                 random_state=420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zach/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py:267: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params)\n"
     ]
    }
   ],
   "source": [
    "# best params\n",
    "\n",
    "# # best params - no overfitting\n",
    "# Random Forest Training Accuracy 0.8507482229704452\n",
    "# Random Forest Validation Accuracy 0.8053872053872054\n",
    "# rf = RandomForestClassifier(n_estimators=2000, \n",
    "#                            criterion='entropy',\n",
    "#                            max_depth=50, \n",
    "#                            min_samples_leaf = 2 \n",
    "#                            )\n",
    "\n",
    "# initialize and fit\n",
    "rf = RandomForestClassifier(n_estimators=3000, \n",
    "                           criterion='entropy',\n",
    "                           max_depth=50, \n",
    "                           min_samples_leaf = 3 # 2 keeps the tree from overfitting, 1 is spicy\n",
    "                           )\n",
    "pipe_rf = make_pipeline(ce.OneHotEncoder(use_cat_names=True),\n",
    "                       rf).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_train = pipe_rf.predict(X_train)\n",
    "rf_val = pipe_rf.predict(X_val)\n",
    "rf_test = pipe_rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Training Accuracy 0.8200149644594089\n",
      "Random Forest Validation Accuracy 0.7936026936026936\n"
     ]
    }
   ],
   "source": [
    "print ('Random Forest Training Accuracy', accuracy_score(y_train, rf_train))\n",
    "print ('Random Forest Validation Accuracy', accuracy_score(y_val, rf_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_val_prob = pipe_rf.predict_proba(X_val)\n",
    "rf_test_prob = pipe_rf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation and Kaggle Submission\n",
    "\n",
    "### Validation\n",
    "\n",
    "Lastly, we want to evaluate our model on validation data, and submit to Kaggle if appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict with XGB\n",
    "xgb_val_classes = xgb_le.inverse_transform(np.asarray([np.argmax(line) for line in xgb_val]))\n",
    "xgb_test_classes = xgb_le.inverse_transform(np.asarray([np.argmax(line) for line in xgb_test]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Classification Report:\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             functional       0.82      0.90      0.86      3226\n",
      "functional needs repair       0.67      0.35      0.46       432\n",
      "         non functional       0.84      0.80      0.82      2282\n",
      "\n",
      "              micro avg       0.82      0.82      0.82      5940\n",
      "              macro avg       0.78      0.68      0.71      5940\n",
      "           weighted avg       0.82      0.82      0.81      5940\n",
      "\n",
      "Accuracy Scores\n",
      "\n",
      "XGBoost Accuracy: 0.8217171717171717\n"
     ]
    }
   ],
   "source": [
    "# compare classification reports for xgb alone vs. combined\n",
    "y_val = xgb_le.inverse_transform(y_train_clean_val.ravel())\n",
    "print ('XGBoost Classification Report:\\n', classification_report(y_val, xgb_val_classes))\n",
    "print ('Accuracy Scores\\n')\n",
    "print ('XGBoost Accuracy:', accuracy_score(y_val, xgb_val_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGboost (xgb_val) + rf predictions (rf_val_prob) - test on validation\n",
    "ensemble_val = np.sum([xgb_val, rf_val_prob], axis=0)\n",
    "ensemble_val_pred = xgb_le.inverse_transform(np.asarray([np.argmax(line) for line in ensemble_val]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost + RF Accuracy: 0.8198653198653199\n",
      "XGBoost + RF Classification Report\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             functional       0.80      0.92      0.86      3226\n",
      "functional needs repair       0.77      0.28      0.41       432\n",
      "         non functional       0.85      0.78      0.82      2282\n",
      "\n",
      "              micro avg       0.82      0.82      0.82      5940\n",
      "              macro avg       0.81      0.66      0.69      5940\n",
      "           weighted avg       0.82      0.82      0.81      5940\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print ('XGBoost + RF Accuracy:', accuracy_score(y_val, ensemble_val_pred))\n",
    "print ('XGBoost + RF Classification Report\\n', classification_report(y_val, ensemble_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict test data\n",
    "ensemble_test = np.sum([xgb_test, rf_test_prob], axis=0)\n",
    "ensemble_test_pred = xgb_le.inverse_transform(np.asarray([np.argmax(line) for line in ensemble_test]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submit = pd.DataFrame(data=ensemble_test_pred, index=X_test.index, columns=['status_group'])\n",
    "df_submit.to_csv('xgboost_rf_v6.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: KAGGLE_CONFIG_DIR=/Users/zach/Kaggle\n",
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /Users/zach/Kaggle/kaggle.json'\n",
      "100%|█████████████████████████████████████████| 264k/264k [00:01<00:00, 232kB/s]\n",
      "Successfully submitted to DS1 Predictive Modeling Challenge"
     ]
    }
   ],
   "source": [
    "# SUBMIT!\n",
    "%env KAGGLE_CONFIG_DIR=/Users/zach/Kaggle\n",
    "!kaggle competitions submit -c ds1-predictive-modeling-challenge -f xgboost_rf_v6.csv -m 'xgboost classifier and random forest classifier, rf trained on validation data too'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
