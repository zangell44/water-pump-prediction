{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = 999\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, precision_score\n",
    "from sklearn.metrics import recall_score, f1_score, confusion_matrix\n",
    "from sklearn.metrics import classification_report, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV, validation_curve, learning_curve\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import category_encoders as ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_installer(installer):\n",
    "    \n",
    "    unknown = ['0', 'unknown']\n",
    "    \n",
    "    if installer in unknown:\n",
    "        return 'unknown'\n",
    "    \n",
    "    government = ['government ', 'government', 'dwe', 'hesawa', 'rwe', 'central government', 'lga',\n",
    "                 'district council', 'gover', 'gove', 'gov', 'district water department',\n",
    "                 'sengerema water department', 'distri', 'centr', 'distric water department',\n",
    "                 'tasaf']\n",
    "    \n",
    "    if installer in government:\n",
    "        return 'government'\n",
    "    \n",
    "    community = ['community', 'commu', 'villagers', 'twesa']\n",
    "    \n",
    "    if installer in community:\n",
    "        return 'community'\n",
    "    \n",
    "    religious = ['church of disciples', 'kkkt', 'world vision', 'rc church', 'rc', 'tcrs',\n",
    "                'dmdd']\n",
    "    \n",
    "    if installer in religious:\n",
    "        return 'religious'\n",
    "    \n",
    "    international = ['norad', 'fini water', 'danida', 'danid', 'ces', 'kuwait',\n",
    "                    'finw']\n",
    "    \n",
    "    if installer in international:\n",
    "        return 'international'\n",
    "    \n",
    "    private = ['private', 'privat', 'kiliwater', 'wedeco']\n",
    "    \n",
    "    if installer in private:\n",
    "        return 'private'\n",
    "    \n",
    "    aid = ['roman', 'amref', 'world bank', 'unicef', 'oxfam']\n",
    "    \n",
    "    if installer in aid:\n",
    "        return 'aid'\n",
    "    \n",
    "    \n",
    "    return 'other'\n",
    "\n",
    "def season(month):\n",
    "    \"\"\"\n",
    "    Returns a string corresponding to the typical seasononal period in Tanzania, given\n",
    "    month input as an integer\n",
    "    \"\"\"\n",
    "    if month in [4,5]:\n",
    "        return 'heavy_rain'\n",
    "    elif month in [12, 1, 2]:\n",
    "        return 'hot_dry'\n",
    "    elif month == 3:\n",
    "        return 'intermittent_rain'\n",
    "    elif month in [6,7,8,9,10]:\n",
    "        return 'cool_dry'\n",
    "    else:\n",
    "        return 'short_rains'\n",
    "\n",
    "def wrangle(X, X_train):\n",
    "    \"\"\"\n",
    "    Takes in the raw water pump features and returns an enhanced dataframe\n",
    "    \n",
    "    We also need to pass in the training data for some functions, such as getting top value_counts.\n",
    "    \n",
    "    When wrangling the training set, we will pass two copies of the training data\n",
    "    \n",
    "    \"\"\"\n",
    "    X = X.copy()\n",
    "    X_train = X_train.copy()\n",
    "    \n",
    "    ### date recorded ###\n",
    "    X['year_recorded'] = X['date_recorded'].apply(lambda x: int(x.split('-')[0]))\n",
    "    X['years_since_construction'] = [record - construction if construction > 1900 \n",
    "                                        else 100 for record, construction in \n",
    "                                        zip(X['year_recorded'], X['construction_year'])]\n",
    "    \n",
    "    days = X_train['date_recorded'].value_counts()\n",
    "    X['day_record_count'] = [days[day] if day in days.index else 0 for day in X['date_recorded']]\n",
    "    X['month_recorded'] = X['date_recorded'].apply(lambda x: int(x.split('-')[1]))\n",
    "    X['season'] = X['month_recorded'].apply(season)\n",
    "    \n",
    "    ### funder / installer ###\n",
    "    # convert all strings to lowercase\n",
    "    X['funder'], X['installer'] = X['funder'].str.lower(), X['installer'].str.lower()\n",
    "    X['funded_and_installed'] = np.where(X['funder'] == X['installer'], True, False)\n",
    "    # encode into categories\n",
    "    X['funder_cat'] = X['funder'].apply(map_installer)\n",
    "    X['installer_cat'] = X['installer'].apply(map_installer)\n",
    "    \n",
    "    # drop full feature sets\n",
    "    X = X.drop(['funder', 'installer'], axis=1)\n",
    "    \n",
    "#     top_funders, top_installers = X_train['funder'].str.lower().value_counts().index[:20], X_train['installer'].str.lower().value_counts().index[:20]\n",
    "#     X['funder'] = ['other' if funder not in top_funders else funder for funder in X['funder']]\n",
    "#     X['installer'] = ['other' if installer not in top_installers else installer for installer in X['installer']]\n",
    "    \n",
    "    ### total static head ###\n",
    "    X['tsh_zero'] = X['amount_tsh'] == 0.0\n",
    "#     X['log_amount_tsh'] = np.log(X['amount_tsh']) <- has problems with 0\n",
    "    \n",
    "    ### waterpoint name ###\n",
    "    X['wpt_name'] = X['wpt_name'].str.lower()\n",
    "    top_wpt = X_train['wpt_name'].str.lower().value_counts().index[:50]\n",
    "    X['wpt_name'] = ['other' if wpt_name not in top_wpt else wpt_name for wpt_name in X['wpt_name'].str.lower()]\n",
    "    \n",
    "    ### population ###\n",
    "    X['population_unknown'] = X['population'] == 0\n",
    "    X['population'].replace(0, np.nan, inplace=True)\n",
    "    X[\"population\"].fillna(X_train.groupby(['region', 'district_code'])[\"population\"].transform(\"median\"), inplace=True)\n",
    "\n",
    "    X[\"population\"].fillna(X_train.groupby(['region'])[\"population\"].transform(\"median\"), inplace=True)\n",
    "\n",
    "    X[\"population\"].fillna(X_train[\"population\"].median(), inplace=True)\n",
    "\n",
    "    \n",
    "    ### subvillage ###\n",
    "    # encode unknown as binary\n",
    "    X['subvillage_unknown'] = X['subvillage'].isnull()\n",
    "    # encode top 50, everything else as other\n",
    "    X['subvillage'] = X['subvillage'].str.lower()\n",
    "    sub_villages_count = X_train['subvillage'].str.lower().value_counts()\n",
    "    X['subvillage_waterpoints'] = [sub_villages_count[vill] if vill in sub_villages_count.index else 0 for vill in X['subvillage'].str.lower()]\n",
    "    \n",
    "    top_subvillage = X_train['subvillage'].str.lower().value_counts().index[:50]\n",
    "    X['subvillage'] = ['other' if vill not in top_subvillage else vill for vill in X['subvillage'].str.lower()]\n",
    "    \n",
    "    ### region code ###\n",
    "    # encode region codes as strings, which will be automatically one hot encoded later\n",
    "    X['region_code'] = X['region_code'].astype(str)\n",
    "    X = X.drop('region', axis=1)\n",
    "    \n",
    "    ### district code ###\n",
    "    # encode district codes as strings, which will be automatically one hot encoded later\n",
    "    X['district_code'] = X['district_code'].astype(str)\n",
    "    \n",
    "    ### lga ###\n",
    "    X['lga'] = X['lga'].str.lower()\n",
    "    # encode urban and rural\n",
    "    X['lga_rural'] = [lga.find('rural') != -1 for lga in X['lga']]\n",
    "    X['lga_urban'] = [lga.find('urban') != -1 for lga in X['lga']]\n",
    "    # encode anything except the top 30 as other\n",
    "    top_lga = X_train['lga'].str.lower().value_counts().index[:30]\n",
    "    X['lga'] = ['other' if lga not in top_lga else lga for lga in X['lga'].str.lower()]\n",
    "    \n",
    "    ### ward ###\n",
    "    # create new variable with the number of waterpoints in a given ward\n",
    "    ward_counts = X_train['ward'].str.lower().value_counts()\n",
    "    X['ward_wpt_count'] = [ward_counts[ward] if ward in ward_counts.index else 0 for ward in X['ward'].str.lower()]\n",
    "    \n",
    "    \n",
    "    ### public meeting ###\n",
    "    X['public_meeting_unknown'] = X['public_meeting'].isnull() # encode missing vals to a binary var\n",
    "    X['public_meeting'] = X['public_meeting'].fillna(False) # fill na with false\n",
    "    \n",
    "    ### recorded by ###\n",
    "    X = X.drop('recorded_by', axis=1) # drop it, all the same value\n",
    "    \n",
    "    ### permit ###\n",
    "    X['permit_unknown'] = X['permit'].isnull() # encode missing vals to a binary var\n",
    "    X['permit'] = X['permit'].fillna(False) # fill na with false\n",
    "    \n",
    "    ### scheme name/management ###\n",
    "    X['scheme_management'] = X['scheme_management'].fillna('None') # encode nulls as 'None'\n",
    "    X = X.drop('scheme_name', axis=1)\n",
    "    \n",
    "    ### construction year ###\n",
    "    X['construction_unknown'] = X['construction_year'] < 1900\n",
    "#     X['early60s'] = X['construction_year'].between(1900, 1964)\n",
    "#     X['late60s'] = X['construction_year'].between(1965, 1969)\n",
    "#     X['early70s'] = X['construction_year'].between(1970, 1974)\n",
    "#     X['late70s'] = X['construction_year'].between(1975, 1979)\n",
    "#     X['early80s'] = X['construction_year'].between(1980, 1984)\n",
    "#     X['late80s'] = X['construction_year'].between(1985, 1989)\n",
    "#     X['early90s'] = X['construction_year'].between(1990, 1994)\n",
    "#     X['late90s'] = X['construction_year'].between(1995, 1999)\n",
    "#     X['early00s'] = X['construction_year'].between(2000, 2004)\n",
    "#     X['late00s'] = X['construction_year'].between(2005, 2009)\n",
    "#     X['early10s'] = X['construction_year'].between(2010, 2014)\n",
    "#     X = X.drop('construction_year', axis=1)\n",
    "    \n",
    "    ### extraction type ###\n",
    "    X = X.drop(['extraction_type_group', 'extraction_type_class'], axis=1)\n",
    "    \n",
    "    ### management ###\n",
    "    X = X.drop('management_group', axis=1)\n",
    "    \n",
    "    ### payment ###\n",
    "    X = X.drop('payment_type', axis=1)\n",
    "    \n",
    "    ### quality ###\n",
    "    X = X.drop(['quality_group', 'quantity_group'], axis=1)\n",
    "    \n",
    "    ### source ###\n",
    "    X = X.drop(['source_type', 'source_class'], axis=1)\n",
    "    \n",
    "    ### waterpoint type ###\n",
    "    X = X.drop('waterpoint_type_group', axis=1)\n",
    "    \n",
    "    ### longitude/latitude/gps_height ###\n",
    "    X['gps_height_bad'] = X['gps_height'] <= 0.0 # min height should be sea level\n",
    "    X['latitude_bad'] = X['latitude'] < 25.0\n",
    "    X['longitude_bad'] = X['longitude'] > -0.5\n",
    "    \n",
    "    # fill in missing values based on location - STOLEN FROM BREADWARD\n",
    "    \n",
    "#     training_data[\"gps_height\"].fillna(training_data.groupby(['region', 'district_code'])[\"gps_height\"].transform(\"mean\"), inplace=True)\n",
    "#     training_data[\"gps_height\"].fillna(training_data.groupby(['region'])[\"gps_height\"].transform(\"mean\"), inplace=True)\n",
    "#     training_data[\"gps_height\"].fillna(training_data[\"gps_height\"].mean(), inplace=True)\n",
    "#     training_data[\"amount_tsh\"].fillna(training_data.groupby(['region', 'district_code'])[\"amount_tsh\"].transform(\"median\"), inplace=True)\n",
    "#     training_data[\"amount_tsh\"].fillna(training_data.groupby(['region'])[\"amount_tsh\"].transform(\"median\"), inplace=True)\n",
    "#     training_data[\"amount_tsh\"].fillna(training_data[\"amount_tsh\"].median(), inplace=True)\n",
    "#     training_data[\"latitude\"].fillna(training_data.groupby(['region', 'district_code'])[\"latitude\"].transform(\"mean\"), inplace=True)\n",
    "#     training_data[\"longitude\"].fillna(training_data.groupby(['region', 'district_code'])[\"longitude\"].transform(\"mean\"), inplace=True)\n",
    "#     training_data[\"longitude\"].fillna(training_data.groupby(['region'])[\"longitude\"].transform(\"mean\"), inplace=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ### DROPPING AFTER FEATURE ENGINEERING ####\n",
    "    X = X.drop('date_recorded', axis=1)\n",
    "  \n",
    "    return X # returned wrangled dataframe\n",
    "\n",
    "df_raw = pd.read_csv('train_features.csv', index_col=0)\n",
    "df_test_raw = pd.read_csv('test_features.csv', index_col=0)\n",
    "X_train, X_test = wrangle(df_raw, df_raw), wrangle(df_test_raw, df_raw)\n",
    "y_train = pd.read_csv('train_labels.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # input KNN predictions based on geography\n",
    "# include_knn = False\n",
    "# if include_knn:\n",
    "#     from sklearn.neighbors import KNeighborsClassifier\n",
    "#     geo = ['latitude', 'longitude']\n",
    "#     knn = KNeighborsClassifier(n_neighbors=5).fit(X_train[geo], y_train)\n",
    "#     geo_train = knn.predict_proba(X_train[geo])\n",
    "#     geo_test = knn.predict_proba(X_test[geo])\n",
    "\n",
    "#     X_train['knn_func'], X_train['knn_non_func'], X_train['knn_repair'] = geo_train[:,0], geo_train[:,1], geo_train[:,2]\n",
    "#     X_test['knn_func'], X_test['knn_non_func'], X_test['knn_repair'] = geo_test[:,0], geo_test[:,1], geo_test[:,2]\n",
    "\n",
    "# encoding categoricals\n",
    "cat_encoder = ce.OneHotEncoder(use_cat_names=True)\n",
    "cat_encoder.fit(X_train, y_train)\n",
    "X_train_clean = cat_encoder.transform(X_train)\n",
    "X_test_clean = cat_encoder.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode labels\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_train_clean = le.fit_transform(y_train.values.ravel())\n",
    "\n",
    "X_train_clean_train, X_train_clean_val, y_train_clean_train, y_train_clean_val = train_test_split(X_train_clean,\n",
    "                                                                                                 y_train_clean,\n",
    "                                                                                                 test_size=0.15,\n",
    "                                                                                                 stratify=y_train_clean,\n",
    "                                                                                                 shuffle=True)\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train_clean_train, label=y_train_clean_train)\n",
    "dval = xgb.DMatrix(X_train_clean_val, label=y_train_clean_val)\n",
    "dtest = xgb.DMatrix(X_test_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and wrangle\n",
    "df_raw = pd.read_csv('train_features.csv', index_col=0)\n",
    "df_test_raw = pd.read_csv('test_features.csv', index_col=0)\n",
    "X_train, X_test = wrangle(df_raw, df_raw), wrangle(df_test_raw, df_raw)\n",
    "y_train = pd.read_csv('train_labels.csv', index_col=0)\n",
    "\n",
    "# encode labels\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_train_clean = le.fit_transform(y_train.values.ravel())\n",
    "\n",
    "# encoding categoricals\n",
    "cat_encoder = ce.OneHotEncoder(use_cat_names=True)\n",
    "cat_encoder.fit(X_train, y_train)\n",
    "X_train_clean = cat_encoder.transform(X_train)\n",
    "X_test_clean = cat_encoder.transform(X_test)\n",
    "\n",
    "X_train_clean_train, X_train_clean_val, y_train_clean_train, y_train_clean_val = train_test_split(X_train_clean,\n",
    "                                                                                                 y_train_clean,\n",
    "                                                                                                 test_size=0.10,\n",
    "                                                                                                 stratify=y_train_clean,\n",
    "                                                                                                 shuffle=True,\n",
    "                                                                                                 random_state=420)\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train_clean_train, label=y_train_clean_train)\n",
    "dval = xgb.DMatrix(X_train_clean_val, label=y_train_clean_val)\n",
    "dtest = xgb.DMatrix(X_test_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-merror:0.245211\tvalidation-merror:0.271549\n",
      "Multiple eval metrics have been passed: 'validation-merror' will be used for early stopping.\n",
      "\n",
      "Will train until validation-merror hasn't improved in 30 rounds.\n",
      "[1]\ttrain-merror:0.216162\tvalidation-merror:0.244949\n",
      "[2]\ttrain-merror:0.200355\tvalidation-merror:0.23064\n",
      "[3]\ttrain-merror:0.18743\tvalidation-merror:0.219529\n",
      "[4]\ttrain-merror:0.182211\tvalidation-merror:0.214478\n",
      "[5]\ttrain-merror:0.173962\tvalidation-merror:0.202357\n",
      "[6]\ttrain-merror:0.173363\tvalidation-merror:0.204714\n",
      "[7]\ttrain-merror:0.169921\tvalidation-merror:0.202189\n",
      "[8]\ttrain-merror:0.168967\tvalidation-merror:0.200505\n",
      "[9]\ttrain-merror:0.166648\tvalidation-merror:0.198822\n",
      "[10]\ttrain-merror:0.166049\tvalidation-merror:0.201347\n",
      "[11]\ttrain-merror:0.164478\tvalidation-merror:0.19899\n",
      "[12]\ttrain-merror:0.164141\tvalidation-merror:0.198653\n",
      "[13]\ttrain-merror:0.163412\tvalidation-merror:0.2\n",
      "[14]\ttrain-merror:0.162514\tvalidation-merror:0.200168\n",
      "[15]\ttrain-merror:0.160494\tvalidation-merror:0.200168\n",
      "[16]\ttrain-merror:0.159428\tvalidation-merror:0.198485\n",
      "[17]\ttrain-merror:0.1581\tvalidation-merror:0.19798\n",
      "[18]\ttrain-merror:0.157632\tvalidation-merror:0.198316\n",
      "[19]\ttrain-merror:0.15737\tvalidation-merror:0.197811\n",
      "[20]\ttrain-merror:0.155855\tvalidation-merror:0.197306\n",
      "[21]\ttrain-merror:0.15593\tvalidation-merror:0.197138\n",
      "[22]\ttrain-merror:0.154284\tvalidation-merror:0.19596\n",
      "[23]\ttrain-merror:0.152319\tvalidation-merror:0.194613\n",
      "[24]\ttrain-merror:0.151721\tvalidation-merror:0.194949\n",
      "[25]\ttrain-merror:0.150224\tvalidation-merror:0.194108\n",
      "[26]\ttrain-merror:0.150037\tvalidation-merror:0.192424\n",
      "[27]\ttrain-merror:0.149289\tvalidation-merror:0.193603\n",
      "[28]\ttrain-merror:0.147924\tvalidation-merror:0.192088\n",
      "[29]\ttrain-merror:0.146745\tvalidation-merror:0.192088\n",
      "[30]\ttrain-merror:0.145829\tvalidation-merror:0.189899\n",
      "[31]\ttrain-merror:0.144463\tvalidation-merror:0.189562\n",
      "[32]\ttrain-merror:0.142817\tvalidation-merror:0.189731\n",
      "[33]\ttrain-merror:0.141134\tvalidation-merror:0.191077\n",
      "[34]\ttrain-merror:0.14018\tvalidation-merror:0.191751\n",
      "[35]\ttrain-merror:0.138664\tvalidation-merror:0.190909\n",
      "[36]\ttrain-merror:0.137617\tvalidation-merror:0.191582\n",
      "[37]\ttrain-merror:0.136831\tvalidation-merror:0.191077\n",
      "[38]\ttrain-merror:0.136008\tvalidation-merror:0.189226\n",
      "[39]\ttrain-merror:0.135503\tvalidation-merror:0.188552\n",
      "[40]\ttrain-merror:0.135092\tvalidation-merror:0.188552\n",
      "[41]\ttrain-merror:0.133707\tvalidation-merror:0.1867\n",
      "[42]\ttrain-merror:0.132623\tvalidation-merror:0.186364\n",
      "[43]\ttrain-merror:0.131799\tvalidation-merror:0.187205\n",
      "[44]\ttrain-merror:0.1315\tvalidation-merror:0.186364\n",
      "[45]\ttrain-merror:0.131369\tvalidation-merror:0.186195\n",
      "[46]\ttrain-merror:0.13021\tvalidation-merror:0.1867\n",
      "[47]\ttrain-merror:0.129349\tvalidation-merror:0.187374\n",
      "[48]\ttrain-merror:0.128114\tvalidation-merror:0.185522\n",
      "[49]\ttrain-merror:0.126861\tvalidation-merror:0.185859\n",
      "[50]\ttrain-merror:0.126562\tvalidation-merror:0.184343\n",
      "[51]\ttrain-merror:0.125758\tvalidation-merror:0.183838\n",
      "[52]\ttrain-merror:0.125365\tvalidation-merror:0.18266\n",
      "[53]\ttrain-merror:0.124804\tvalidation-merror:0.184512\n",
      "[54]\ttrain-merror:0.124037\tvalidation-merror:0.184343\n",
      "[55]\ttrain-merror:0.122802\tvalidation-merror:0.183165\n",
      "[56]\ttrain-merror:0.122241\tvalidation-merror:0.182492\n",
      "[57]\ttrain-merror:0.121437\tvalidation-merror:0.18266\n",
      "[58]\ttrain-merror:0.120501\tvalidation-merror:0.182323\n",
      "[59]\ttrain-merror:0.119454\tvalidation-merror:0.182323\n",
      "[60]\ttrain-merror:0.119473\tvalidation-merror:0.182997\n",
      "[61]\ttrain-merror:0.118612\tvalidation-merror:0.182997\n",
      "[62]\ttrain-merror:0.118126\tvalidation-merror:0.184007\n",
      "[63]\ttrain-merror:0.11734\tvalidation-merror:0.18266\n",
      "[64]\ttrain-merror:0.117022\tvalidation-merror:0.182492\n",
      "[65]\ttrain-merror:0.115619\tvalidation-merror:0.18367\n",
      "[66]\ttrain-merror:0.115451\tvalidation-merror:0.181987\n",
      "[67]\ttrain-merror:0.114572\tvalidation-merror:0.18367\n",
      "[68]\ttrain-merror:0.114104\tvalidation-merror:0.18165\n",
      "[69]\ttrain-merror:0.113599\tvalidation-merror:0.184007\n",
      "[70]\ttrain-merror:0.113\tvalidation-merror:0.183333\n",
      "[71]\ttrain-merror:0.112533\tvalidation-merror:0.183333\n",
      "[72]\ttrain-merror:0.112065\tvalidation-merror:0.184343\n",
      "[73]\ttrain-merror:0.111728\tvalidation-merror:0.184343\n",
      "[74]\ttrain-merror:0.11098\tvalidation-merror:0.184848\n",
      "[75]\ttrain-merror:0.109727\tvalidation-merror:0.18569\n",
      "[76]\ttrain-merror:0.109278\tvalidation-merror:0.185185\n",
      "[77]\ttrain-merror:0.109053\tvalidation-merror:0.183838\n",
      "[78]\ttrain-merror:0.108511\tvalidation-merror:0.184175\n",
      "[79]\ttrain-merror:0.108081\tvalidation-merror:0.183165\n",
      "[80]\ttrain-merror:0.107819\tvalidation-merror:0.183502\n",
      "[81]\ttrain-merror:0.107202\tvalidation-merror:0.183502\n",
      "[82]\ttrain-merror:0.106753\tvalidation-merror:0.18367\n",
      "[83]\ttrain-merror:0.106416\tvalidation-merror:0.182997\n",
      "[84]\ttrain-merror:0.106023\tvalidation-merror:0.183333\n",
      "[85]\ttrain-merror:0.105649\tvalidation-merror:0.183333\n",
      "[86]\ttrain-merror:0.105369\tvalidation-merror:0.181818\n",
      "[87]\ttrain-merror:0.104901\tvalidation-merror:0.18165\n",
      "[88]\ttrain-merror:0.104227\tvalidation-merror:0.180976\n",
      "[89]\ttrain-merror:0.103909\tvalidation-merror:0.18165\n",
      "[90]\ttrain-merror:0.103273\tvalidation-merror:0.182492\n",
      "[91]\ttrain-merror:0.102937\tvalidation-merror:0.183333\n",
      "[92]\ttrain-merror:0.102189\tvalidation-merror:0.182155\n",
      "[93]\ttrain-merror:0.102095\tvalidation-merror:0.182155\n",
      "[94]\ttrain-merror:0.101459\tvalidation-merror:0.182155\n",
      "[95]\ttrain-merror:0.100524\tvalidation-merror:0.18266\n",
      "[96]\ttrain-merror:0.100449\tvalidation-merror:0.181481\n",
      "[97]\ttrain-merror:0.100561\tvalidation-merror:0.181987\n",
      "[98]\ttrain-merror:0.100262\tvalidation-merror:0.18165\n",
      "[99]\ttrain-merror:0.099925\tvalidation-merror:0.180303\n",
      "[100]\ttrain-merror:0.09957\tvalidation-merror:0.179966\n",
      "[101]\ttrain-merror:0.098616\tvalidation-merror:0.180808\n",
      "[102]\ttrain-merror:0.098129\tvalidation-merror:0.180135\n",
      "[103]\ttrain-merror:0.097793\tvalidation-merror:0.180808\n",
      "[104]\ttrain-merror:0.097344\tvalidation-merror:0.180976\n",
      "[105]\ttrain-merror:0.09682\tvalidation-merror:0.181145\n",
      "[106]\ttrain-merror:0.096614\tvalidation-merror:0.181145\n",
      "[107]\ttrain-merror:0.096502\tvalidation-merror:0.18165\n",
      "[108]\ttrain-merror:0.095885\tvalidation-merror:0.181145\n",
      "[109]\ttrain-merror:0.095866\tvalidation-merror:0.181313\n",
      "[110]\ttrain-merror:0.095473\tvalidation-merror:0.179798\n",
      "[111]\ttrain-merror:0.094725\tvalidation-merror:0.179461\n",
      "[112]\ttrain-merror:0.094557\tvalidation-merror:0.18064\n",
      "[113]\ttrain-merror:0.093752\tvalidation-merror:0.179293\n",
      "[114]\ttrain-merror:0.093322\tvalidation-merror:0.181481\n",
      "[115]\ttrain-merror:0.093023\tvalidation-merror:0.179125\n",
      "[116]\ttrain-merror:0.092836\tvalidation-merror:0.178451\n",
      "[117]\ttrain-merror:0.09263\tvalidation-merror:0.178788\n",
      "[118]\ttrain-merror:0.092536\tvalidation-merror:0.178114\n",
      "[119]\ttrain-merror:0.091863\tvalidation-merror:0.178114\n",
      "[120]\ttrain-merror:0.091508\tvalidation-merror:0.177778\n",
      "[121]\ttrain-merror:0.090947\tvalidation-merror:0.178451\n",
      "[122]\ttrain-merror:0.090441\tvalidation-merror:0.178788\n",
      "[123]\ttrain-merror:0.090198\tvalidation-merror:0.17963\n",
      "[124]\ttrain-merror:0.089656\tvalidation-merror:0.179966\n",
      "[125]\ttrain-merror:0.08945\tvalidation-merror:0.18064\n",
      "[126]\ttrain-merror:0.089039\tvalidation-merror:0.181481\n",
      "[127]\ttrain-merror:0.088945\tvalidation-merror:0.180808\n",
      "[128]\ttrain-merror:0.088777\tvalidation-merror:0.180976\n",
      "[129]\ttrain-merror:0.088646\tvalidation-merror:0.17963\n",
      "[130]\ttrain-merror:0.08829\tvalidation-merror:0.179461\n",
      "[131]\ttrain-merror:0.088047\tvalidation-merror:0.178956\n",
      "[132]\ttrain-merror:0.087654\tvalidation-merror:0.179293\n",
      "[133]\ttrain-merror:0.087243\tvalidation-merror:0.179125\n",
      "[134]\ttrain-merror:0.087299\tvalidation-merror:0.178283\n",
      "[135]\ttrain-merror:0.086869\tvalidation-merror:0.178283\n",
      "[136]\ttrain-merror:0.086775\tvalidation-merror:0.179293\n",
      "[137]\ttrain-merror:0.086457\tvalidation-merror:0.178283\n",
      "[138]\ttrain-merror:0.08642\tvalidation-merror:0.178283\n",
      "[139]\ttrain-merror:0.08612\tvalidation-merror:0.178451\n",
      "[140]\ttrain-merror:0.085391\tvalidation-merror:0.17862\n",
      "[141]\ttrain-merror:0.085428\tvalidation-merror:0.177946\n",
      "[142]\ttrain-merror:0.084923\tvalidation-merror:0.178114\n",
      "[143]\ttrain-merror:0.084605\tvalidation-merror:0.178283\n",
      "[144]\ttrain-merror:0.084512\tvalidation-merror:0.178114\n",
      "[145]\ttrain-merror:0.084287\tvalidation-merror:0.178114\n",
      "[146]\ttrain-merror:0.083259\tvalidation-merror:0.17862\n",
      "[147]\ttrain-merror:0.083015\tvalidation-merror:0.178451\n",
      "[148]\ttrain-merror:0.082735\tvalidation-merror:0.177441\n",
      "[149]\ttrain-merror:0.082884\tvalidation-merror:0.178114\n"
     ]
    }
   ],
   "source": [
    "# v17 run with these params, and WITHOUT season\n",
    "\n",
    "# Stopping. Best iteration:\n",
    "# [107]\ttrain-merror:0.109428\tvalidation-merror:0.180471\n",
    "\n",
    "# param = {'num_class' : 3,\n",
    "#          'scale_pos_weight' : 1,\n",
    "#          'max_depth': 45,\n",
    "#          'eta': 0.1,\n",
    "#          'n_thread' : 4,\n",
    "#          'colsample_bytree' : 0.3,\n",
    "#          'subsample' : 0.3,\n",
    "#          'silent': 1, \n",
    "#          'n_estimators' : 100,\n",
    "#          'reg_alpha' : 0.4,\n",
    "#          'gamma' : 1,\n",
    "#          'objective': 'multi:softprob',\n",
    "#         'eval_metric' : 'merror'} \n",
    "\n",
    "# best params, v19\n",
    "# [148]\ttrain-merror:0.082417\tvalidation-merror:0.18165\n",
    "# Stopping. Best iteration:\n",
    "# [98]\ttrain-merror:0.100224\tvalidation-merror:0.178956\n",
    "# param = {'num_class' : 3,\n",
    "#          'scale_pos_weight' : 1,\n",
    "#          'max_depth': 70, # very deep tree\n",
    "#          'eta': 0.1,\n",
    "#          'n_thread' : 4,\n",
    "#          'colsample_bytree' : 0.4,\n",
    "#          'subsample' : 0.3,\n",
    "#          'silent': 1, \n",
    "#          'n_estimators' : 1000, # lots of estimators\n",
    "#          'reg_alpha' : 0.3,\n",
    "#          'gamma' : 1,\n",
    "#          'objective': 'multi:softprob',\n",
    "#         'eval_metric' : 'merror'} \n",
    "\n",
    "# setting parameters \n",
    "param = {'num_class' : 3,\n",
    "         'scale_pos_weight' : 1,\n",
    "         'max_depth': 80, # very deep tree\n",
    "         'eta': 0.1,\n",
    "         'n_thread' : 4,\n",
    "         'colsample_bytree' : 0.4,\n",
    "         'subsample' : 0.3,\n",
    "         'silent': 1, \n",
    "         'n_estimators' : 2000, # lots of estimators\n",
    "         'reg_alpha' : 0.3,\n",
    "         'gamma' : 1,\n",
    "         'objective': 'multi:softprob',\n",
    "        'eval_metric' : 'merror'} \n",
    "num_rounds = 150\n",
    "eval_list = [(dtrain, 'train'), (dval, 'validation')]\n",
    "\n",
    "bst = xgb.train(param, dtrain, num_rounds, eval_list, early_stopping_rounds=30)\n",
    "bst.save_model('xgboost_iteration21_v0.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_rounds = 100\n",
    "# bst_v1 = xgb.train(param, dtrain, num_rounds, eval_list, xgb_model='xgboost_iteration18_v0.model')\n",
    "# bst_v1.save_model('xgboost_iteration18_v1.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict test data\n",
    "preds = bst.predict(dtest)\n",
    "# extracting most confident predictions\n",
    "best_preds = le.inverse_transform(np.asarray([np.argmax(line) for line in preds]))\n",
    "# create df to hold submission\n",
    "df_boost = pd.DataFrame(data=best_preds, index=X_test.index, columns=['status_group'])\n",
    "df_boost.to_csv('xgboost_iteration21_v0_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: KAGGLE_CONFIG_DIR=/Users/zach/Kaggle\n",
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /Users/zach/Kaggle/kaggle.json'\n",
      "100%|█████████████████████████████████████████| 264k/264k [00:01<00:00, 214kB/s]\n",
      "Successfully submitted to DS1 Predictive Modeling Challenge"
     ]
    }
   ],
   "source": [
    "# SUBMIT!\n",
    "%env KAGGLE_CONFIG_DIR=/Users/zach/Kaggle\n",
    "!kaggle competitions submit -c ds1-predictive-modeling-challenge -f xgboost_iteration21_v0_submission.csv -m 'xgboost classifier final submit'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy:  0.9175832398054621\n",
      "Validation accuracy:  0.8183501683501684\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "             functional       0.82      0.90      0.85      3226\n",
      "functional needs repair       0.65      0.34      0.45       432\n",
      "         non functional       0.84      0.80      0.82      2282\n",
      "\n",
      "              micro avg       0.82      0.82      0.82      5940\n",
      "              macro avg       0.77      0.68      0.71      5940\n",
      "           weighted avg       0.81      0.82      0.81      5940\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lets take a look at the classification report on the validation data\n",
    "train_preds = le.inverse_transform(np.asarray([np.argmax(line) for line in bst.predict(dtrain)]))\n",
    "val_preds = le.inverse_transform(np.asarray([np.argmax(line) for line in bst.predict(dval)]))\n",
    "print ('Training accuracy: ', accuracy_score(le.inverse_transform(y_train_clean_train), train_preds))\n",
    "print ('Validation accuracy: ', accuracy_score(le.inverse_transform(y_train_clean_val), val_preds))\n",
    "print (classification_report(le.inverse_transform(y_train_clean_val), val_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
